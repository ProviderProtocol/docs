---
title: Embedding Interface
description: UPP Embedding interface for vector embeddings.
---

import { Aside, Badge, Card, CardGrid } from '@astrojs/starlight/components';

# Embedding Interface

<Badge text="UPP-1.2.0" variant="note" />

## Function Signature

```text
embedding(options: EmbeddingOptions) -> EmbeddingInstance
```

## Options

**EmbeddingOptions Structure:**

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `model` | ModelReference | Yes | A model reference from a provider factory |
| `config` | ProviderConfig | No | Provider infrastructure configuration |
| `params` | Map | No | Provider-specific parameters (passed through unchanged) |

## EmbeddingInstance

**EmbeddingInstance Interface:**

| Method/Property | Type | Description |
|-----------------|------|-------------|
| `embed(input, options?)` | Function | Embed one or more inputs |
| `model` | BoundEmbeddingModel | The bound model |
| `params` | Map? | Current parameters |

The `embed()` method is the single interface for all embedding operations. It accepts either a single input or an array of inputs, and optionally supports chunked processing for large-scale operations.

**embed() Overloads:**

```text
// Single or batch input - returns Promise
embed(input: EmbeddingInput | List&lt;EmbeddingInput&gt;) -> Promise&lt;EmbeddingResult&gt;
embed(input: EmbeddingInput | List&lt;EmbeddingInput&gt;, options: EmbedOptions) -> Promise&lt;EmbeddingResult&gt;

// Chunked mode for large-scale - returns EmbeddingStream
embed(input: List&lt;EmbeddingInput&gt;, options: EmbedOptions & { chunked: true }) -> EmbeddingStream
```

**EmbeddingInput Type:**

```text
EmbeddingInput = String | TextBlock | ImageBlock
```

Providers declare which input types they support via `supportedInputs`. Text input is universally supported. Image input is provider-dependent (e.g., Google multimodal embeddings).

**EmbedOptions Structure:**

| Field | Type | Description |
|-------|------|-------------|
| `chunked` | Boolean? | Enable chunked processing with progress (returns EmbeddingStream) |
| `batchSize` | Integer? | Maximum inputs per batch when chunked (default: provider limit) |
| `concurrency` | Integer? | Concurrent batch limit when chunked (default: 1) |
| `signal` | AbortSignal? | Abort signal for cancellation |

When `chunked` is `true`, the method returns an `EmbeddingStream` instead of a Promise. This enables processing of large input sets with progress tracking and incremental results.

## Embedding Results

**EmbeddingResult Structure:**

| Field | Type | Description |
|-------|------|-------------|
| `embeddings` | List&lt;Embedding&gt; | Embeddings in same order as inputs |
| `usage` | EmbeddingUsage | Aggregate usage statistics |
| `metadata` | Map? | Provider-specific response metadata |

**Embedding Structure:**

| Field | Type | Description |
|-------|------|-------------|
| `vector` | List&lt;Float&gt; | The embedding vector |
| `dimensions` | Integer | Vector dimensionality |
| `index` | Integer | Index corresponding to input array position |
| `tokens` | Integer? | Token count for this input (if provider reports) |
| `metadata` | Map? | Provider-specific per-embedding metadata |

**EmbeddingUsage Structure:**

| Field | Type | Description |
|-------|------|-------------|
| `totalTokens` | Integer | Total tokens processed |

## EmbeddingStream

When `embed()` is called with `{ chunked: true }`, it returns an `EmbeddingStream` that provides progress updates during processing.

**EmbeddingStream Interface:**

| Property/Method | Type | Description |
|-----------------|------|-------------|
| `[Symbol.asyncIterator]` | AsyncIterator | Yields `EmbeddingProgress` objects |
| `result` | Promise&lt;EmbeddingResult&gt; | Resolves to complete result after iteration |
| `abort()` | Function | Abort the operation |

**EmbeddingProgress Structure:**

| Field | Type | Description |
|-------|------|-------------|
| `embeddings` | List&lt;Embedding&gt; | Embeddings from the latest batch |
| `completed` | Integer | Total embeddings completed so far |
| `total` | Integer | Total number of inputs |
| `percent` | Float | Percentage complete (0-100) |

## BoundEmbeddingModel

**BoundEmbeddingModel Structure:**

| Field | Type | Description |
|-------|------|-------------|
| `modelId` | String | The model identifier |
| `provider` | EmbeddingProvider | Reference to the parent provider |
| `maxBatchSize` | Integer | Maximum inputs per batch request |
| `maxInputLength` | Integer | Maximum input length (tokens or characters) |
| `dimensions` | Integer | Default output dimensions (may be configurable via params) |
| `embed(request)` | Function | Execute embedding request |

## Provider Request/Response

Providers implement a single `embed()` method that handles batch requests. The `embedding()` core handles input normalization and chunked processing.

**EmbeddingRequest Structure:**

| Field | Type | Description |
|-------|------|-------------|
| `inputs` | List&lt;EmbeddingInput&gt; | Inputs to embed |
| `params` | Map? | Provider-specific parameters (passed through unchanged) |
| `config` | ProviderConfig | Provider infrastructure config |
| `signal` | AbortSignal? | Abort signal for cancellation |

**EmbeddingResponse Structure:**

| Field | Type | Description |
|-------|------|-------------|
| `embeddings` | List&lt;EmbeddingVector&gt; | Embedding vectors |
| `usage` | EmbeddingUsage | Aggregate usage |
| `metadata` | Map? | Provider-specific response metadata |

**EmbeddingVector Structure:**

| Field | Type | Description |
|-------|------|-------------|
| `vector` | List&lt;Float&gt; \| String | The embedding vector (floats or base64-encoded string) |
| `index` | Integer | Index in input array |
| `tokens` | Integer? | Token count for this input |
| `metadata` | Map? | Provider-specific per-embedding metadata |

Providers return `vector` as either a `List&lt;Float&gt;` or a base64-encoded `String` depending on the encoding format requested in params. The `embedding()` core detects the type and normalizes to `List&lt;Float&gt;` in the public result.

## Basic Usage

```text
import { embedding } from "upp"
import openai from "upp/openai"

embedder = embedding({
  model: openai("text-embedding-3-large"),
  config: {
    apiKey: env.OPENAI_API_KEY
  },
  params: {
    dimensions: 1536
  }
})

// Single input
result = await embedder.embed("What is the capital of France?")
print(result.embeddings[0].vector.length)  // 1536
print(result.embeddings[0].dimensions)     // 1536

// Batch input (array)
result = await embedder.embed([
  "First document to embed",
  "Second document to embed",
  "Third document to embed"
])

print(result.embeddings.length)   // 3
print(result.usage.totalTokens)   // Total tokens used
```

## Large-Scale Embedding

For embedding large document sets, use chunked mode to receive progress updates and process results incrementally:

```text
documents = await loadDocuments()  // 10,000 documents

stream = embedder.embed(documents, {
  chunked: true,
  batchSize: 100,
  concurrency: 2
})

for await (progress in stream) {
  print("Progress:", progress.percent + "%")

  // Process embeddings as they complete
  await storeInVectorDB(progress.embeddings)
}

// Get complete result after iteration
final = await stream.result
print("Total tokens:", final.usage.totalTokens)
```

## Provider-Specific Parameters

Parameters are passed through unchanged to the provider's API via the `params` field. Each provider exports its own parameter types enabling full access to provider-native features (e.g., output dimensions, encoding formats, task type hints).

Providers that do not support a parameter MUST ignore it. This enables portable code that includes provider-specific hints without breaking on other providers. Consult individual provider documentation for available parameters.

## Metadata Preservation

Providers MUST preserve provider-specific metadata in the response rather than discarding it. This enables access to provider-native features and diagnostics.

**Per-Embedding Metadata:**

Metadata specific to individual embeddings (e.g., truncation status, token counts):

```text
result = await embedder.embed(["long document..."])

// Google provides truncation info per embedding
print(result.embeddings[0].metadata?.truncated)  // true/false
```

**Response-Level Metadata:**

Metadata applying to the entire response:

```text
// Google may include safety ratings
print(result.metadata?.safetyRatings)
```