---
title: Ollama Provider
description: Ollama local model provider for ProviderProtocol.
---

import { Aside, Badge } from '@astrojs/starlight/components';

# Ollama Provider

Provider for running local models via Ollama.

## Import

```typescript
import { ollama } from '@providerprotocol/ai/ollama';
import type { OllamaLLMParams } from '@providerprotocol/ai/ollama';
```

## Usage

```typescript
import { llm } from '@providerprotocol/ai';
import { ollama } from '@providerprotocol/ai/ollama';

const local = llm({
  model: ollama('llama3.2'),
  system: 'You are a helpful assistant.',
});

const turn = await local.generate('Hello!');
```

## Capabilities

| Capability | Supported |
|------------|-----------|
| Streaming | Yes |
| Tools | Yes |
| Structured Output | Yes |
| Image Input | Yes |
| Audio Input | No |
| Video Input | No |

## Provider-Specific Parameters

```typescript
interface OllamaLLMParams {
  /** Maximum tokens to predict/generate */
  num_predict?: number;

  /** Sampling temperature (higher = more creative) */
  temperature?: number;

  /** Top-p (nucleus) sampling */
  top_p?: number;

  /** Top-k sampling - only sample from top K tokens */
  top_k?: number;

  /** Minimum probability threshold */
  min_p?: number;

  /** Typical-p sampling */
  typical_p?: number;

  /** Repetition penalty (1.0 = disabled) */
  repeat_penalty?: number;

  /** Number of tokens to look back for repeat penalty */
  repeat_last_n?: number;

  /** Presence penalty */
  presence_penalty?: number;

  /** Frequency penalty */
  frequency_penalty?: number;

  /** Mirostat sampling mode (0=disabled, 1=v1, 2=v2) */
  mirostat?: 0 | 1 | 2;

  /** Mirostat learning rate */
  mirostat_eta?: number;

  /** Mirostat target entropy */
  mirostat_tau?: number;

  /** Whether to penalize newlines */
  penalize_newline?: boolean;

  /** Stop sequences */
  stop?: string[];

  /** Random seed for reproducibility */
  seed?: number;

  /** Number of tokens to keep from initial prompt */
  num_keep?: number;

  /** Context window size */
  num_ctx?: number;

  /** Batch size for prompt processing */
  num_batch?: number;

  /** Number of threads to use */
  num_thread?: number;

  /** Number of GPU layers */
  num_gpu?: number;

  /** Main GPU index for multi-GPU */
  main_gpu?: number;

  /** Low VRAM mode */
  low_vram?: boolean;

  /** Use 16-bit floats for KV cache */
  f16_kv?: boolean;

  /** Use memory mapping */
  use_mmap?: boolean;

  /** Lock model in memory */
  use_mlock?: boolean;

  /** Only load vocabulary */
  vocab_only?: boolean;

  /** NUMA optimization mode */
  numa?: boolean;

  /** Tail-free sampling parameter */
  tfs_z?: number;

  /** Enable thinking/reasoning mode */
  think?: boolean | 'high' | 'medium' | 'low';

  /** Keep model loaded in memory (duration string or seconds) */
  keep_alive?: string | number;

  /** Enable log probabilities */
  logprobs?: boolean;

  /** Number of top log probabilities to return */
  top_logprobs?: number;
}
```

<Aside type="note">
Ollama provides extensive control over local inference, including GPU allocation, memory management, and advanced sampling parameters.
</Aside>

## Configuration

```typescript
import { llm } from '@providerprotocol/ai';
import { ollama } from '@providerprotocol/ai/ollama';

const local = llm({
  model: ollama('llama3.2'),
  config: {
    baseUrl: 'http://localhost:11434', // default
    timeout: 120000, // local models may need more time
  },
  params: {
    num_predict: 2048,
    temperature: 0.7,
    num_ctx: 4096,
  },
});
```

<Aside type="tip">
Ollama runs locally so no API key is required. The default base URL is `http://localhost:11434`.
</Aside>

## Keep-Alive

Control how long the model stays loaded in memory:

```typescript
const local = llm({
  model: ollama('llama3.2'),
  params: {
    keep_alive: '5m',   // Keep loaded for 5 minutes
    // keep_alive: 300, // Or specify seconds
    // keep_alive: -1,  // Keep loaded indefinitely
    // keep_alive: 0,   // Unload immediately after request
  },
});
```

## Related

- [llm()](/api/core/llm)
- [Provider Conformance](/spec/providers/conformance)
