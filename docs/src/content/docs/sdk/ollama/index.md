---
title: "ollama"
---

[**@providerprotocol/ai**](../README.md)

***

[@providerprotocol/ai](../modules.md) / ollama

# ollama

Ollama provider for UPP (Unified Provider Protocol)

This module exports the Ollama provider for local model inference.
Ollama runs models locally, eliminating the need for API keys and
external network calls. Ideal for development and privacy-sensitive
applications.

## Remarks

Prerequisites:
1. Install Ollama from https://ollama.ai
2. Pull a model: `ollama pull llama3.2`
3. Ensure Ollama is running: `ollama serve`

## Example

```ts
import { ollama } from '@providerprotocol/ai/ollama';
import { llm } from '@providerprotocol/ai';

// Create an LLM instance with a local model
const model = llm({
  model: ollama('llama3.2'),
  params: { num_predict: 500 }
});

// Generate a response
const turn = await model.generate('Write a haiku about coding.');
console.log(turn.response.text);
```

## Interfaces

- [OllamaLLMParams](interfaces/OllamaLLMParams.md)

## Variables

- [ollama](variables/ollama.md)
