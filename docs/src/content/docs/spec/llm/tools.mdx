---
title: Tools
description: UPP-1.2 tool definition, execution, and strategies.
---

import { Aside, Badge } from '@astrojs/starlight/components';

# Tools

<Badge text="UPP-1.2" variant="note" />

Tools allow LLMs to interact with external systems, execute code, and access real-time information. UPP uses JSON Schema for tool parameter definitions.

## Tool Definition

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `name` | String | Yes | Tool name (must be unique within an llm() instance) |
| `description` | String | Yes | Human-readable description for the model |
| `parameters` | JSONSchema | Yes | JSON Schema defining parameters |
| `run` | Function | Yes | Tool execution function |
| `approval` | Function | No | Optional approval handler for sensitive operations |

### JSONSchema Structure (for tool parameters)

```text
{
  type: "object",
  properties: {
    paramName: {
      type: "string" | "number" | "integer" | "boolean" | "array" | "object",
      description: "Parameter description",
      enum: [...],        // optional
      items: {...},       // for arrays
      properties: {...},  // for nested objects
      required: [...],    // for nested objects
      default: value      // optional
    }
  },
  required: ["paramName", ...]
}
```

## Tool Example

```text
getWeather = {
  name: "getWeather",
  description: "Get current weather for a location",
  parameters: {
    type: "object",
    properties: {
      location: {
        type: "string",
        description: "City name or coordinates"
      },
      units: {
        type: "string",
        enum: ["celsius", "fahrenheit"],
        default: "celsius"
      }
    },
    required: ["location"]
  },
  run: async (params) => {
    weather = await fetchWeather(params.location, params.units ?? "celsius")
    return weather.temp + "° " + params.units + ", " + weather.condition
  }
}

claude = llm({
  model: anthropic("claude-haiku-4-20250514"),
  config: { apiKey: env.ANTHROPIC_API_KEY },
  system: "You are a weather assistant.",
  tools: [getWeather]
})
```

## Tool Approval

For sensitive operations, tools can require approval:

```text
deleteFile = {
  name: "deleteFile",
  description: "Delete a file from the filesystem",
  parameters: {
    type: "object",
    properties: {
      path: { type: "string", description: "File path to delete" }
    },
    required: ["path"]
  },
  approval: async (params) => {
    // UI prompt, admin check, path validation, etc.
    return await promptUser("Allow deletion of " + params.path + "?")
  },
  run: async (params) => {
    await fs.unlink(params.path)
    return "Deleted " + params.path
  }
}
```

## Tool Execution Flow

By default, `llm()` handles tool execution automatically:

1. Model returns an `AssistantMessage` with `toolCalls`
2. If `approval` is defined, it's called first (rejected = error result sent to model)
3. Tool's `run` function is executed with arguments from the model
4. Result (or error) is sent back to the model as `ToolResultMessage`
5. Loop continues until model returns without tool calls OR max iterations reached

### Error Handling

- If `approval()` throws an exception, the exception propagates to the caller and aborts the generation
- If `approval()` returns `false`, an error result is sent to the model
- If the tool's `run` function throws, the error is caught and sent as an error result to the model

<Aside type="caution" title="No Argument Validation">
  `llm()` does NOT validate tool arguments against the JSON Schema. The schema is provided to the model to guide its output, but validation and sanitization of LLM-provided arguments is the responsibility of the tool implementation. Always treat tool arguments as untrusted input.
</Aside>

<Aside type="note" title="Structured Output Validation">
  Similarly, UPP does not validate [structured output](/spec/llm/structured/) responses against their schema. Schemas guide LLM behavior but validation is the application's responsibility in both cases.
</Aside>

## ToolUseStrategy

For custom control over tool execution, including input/output transformation:

| Field | Type | Description |
|-------|------|-------------|
| `maxIterations` | Integer | Maximum tool execution rounds (default: 10) |
| `onToolCall` | Function | Called when the model requests a tool call |
| `onBeforeCall` | Function | Called before tool execution; can skip or transform params |
| `onAfterCall` | Function | Called after tool execution; can transform result |
| `onError` | Function | Called on tool execution error |
| `onMaxIterations` | Function | Called when max iterations reached |

### BeforeCallResult Structure

| Field | Type | Description |
|-------|------|-------------|
| `proceed` | Boolean | Whether to proceed with tool execution |
| `params` | Any? | Transformed parameters to use (optional) |

### AfterCallResult Structure

| Field | Type | Description |
|-------|------|-------------|
| `result` | Any | Transformed result to return to the model |

### Hook Return Types

| Hook | Return Type | Behavior |
|------|-------------|----------|
| `onBeforeCall` | `false` | Skip execution |
| `onBeforeCall` | `true` | Proceed with original params |
| `onBeforeCall` | `BeforeCallResult` | Control execution and optionally transform params |
| `onAfterCall` | `void` | Use original result |
| `onAfterCall` | `AfterCallResult` | Transform result before returning to model |

### Strategy Example

```text
strategy = {
  maxIterations: 5,

  onBeforeCall: async (tool, params) => {
    print("Calling " + tool.name + " with", params)
    return true  // Allow execution with original params
  },

  onAfterCall: async (tool, params, result) => {
    await logToolUsage(tool.name, params, result)
    // Return void to use original result
  },

  onError: async (tool, params, error) => {
    await alertOps("Tool " + tool.name + " failed: " + error.message)
  },

  onMaxIterations: async (iterations) => {
    print("Tool loop hit max iterations:", iterations)
  }
}

claude = llm({
  model: anthropic("claude-haiku-4-20250514"),
  config: { apiKey: env.ANTHROPIC_API_KEY },
  tools: [getWeather, searchWeb],
  toolStrategy: strategy
})
```

### Input Transformation

Transform tool parameters before execution:

```text
strategy = {
  onBeforeCall: async (tool, params) => {
    if (tool.name == "search") {
      // Add default pagination
      return {
        proceed: true,
        params: { ...params, limit: 10, offset: 0 }
      }
    }
    // Proceed with original params for other tools
    return true
  }
}
```

### Output Transformation

Transform tool results before returning to the model:

```text
strategy = {
  onAfterCall: async (tool, params, result) => {
    if (tool.name == "fetch_data") {
      // Sanitize sensitive fields
      return { result: redactPII(result) }
    }
    if (tool.name == "get_users") {
      // Truncate large results
      return { result: result.slice(0, 100) }
    }
    // Return void to use original result
  }
}
```

### Combined Transformation

Input and output transformation can be combined:

```text
strategy = {
  onBeforeCall: async (tool, params) => {
    // Inject authentication
    if (tool.name == "api_call") {
      return {
        proceed: true,
        params: { ...params, authToken: getAuthToken() }
      }
    }
    return true
  },

  onAfterCall: async (tool, params, result) => {
    // Remove injected auth from logged result
    if (tool.name == "api_call") {
      return { result: { ...result, authToken: undefined } }
    }
  }
}
```

## Disabling Automatic Tool Execution

To handle tool calls manually:

```text
claude = llm({
  model: anthropic("claude-haiku-4-20250514"),
  config: { apiKey: env.ANTHROPIC_API_KEY },
  tools: [getWeather],
  toolStrategy: { maxIterations: 0 }  // Disable auto-execution
})

turn = await claude.generate([], "What is the weather?")

if (turn.response.hasToolCalls) {
  // Handle manually
  for toolCall in turn.response.toolCalls {
    print("Model wants to call:", toolCall.toolName)
    // Execute yourself, then continue conversation
  }
}
```

## Multiple Tool Calls

Models may request multiple tool calls in a single response. `llm()` executes them in parallel by default:

```text
claude = llm({
  model: anthropic("claude-haiku-4-20250514"),
  config: { apiKey: env.ANTHROPIC_API_KEY },
  tools: [getWeather, getTime]
})

// Model might call both tools simultaneously
turn = await claude.generate(
  "What is the weather and time in Tokyo and Paris?"
)

// turn.toolExecutions might contain 4 executions
// (weather + time for each city)
```

## Provider-Native Tools

Some providers offer **provider-native tools**—built-in capabilities that execute server-side rather than client-side. These differ fundamentally from UPP function tools:

| Aspect | UPP Function Tools | Provider-Native Tools |
|--------|-------------------|----------------------|
| Passed via | `tools` parameter in `llm()` | `params.tools` (pass-through) |
| Execution | Client-side via `run` function | Server-side by provider |
| Output | Returned by `run()`, sent back as `ToolResultMessage` | Incorporated into response content |
| Definition | Requires `name`, `description`, `parameters`, `run` | Provider-specific structure |

**Examples of provider-native tools:**

- **Web Search** - Search the web for current information
- **Code Interpreter** - Execute code in a sandboxed environment
- **Image Generation** - Generate images from text prompts
- **File Search** - Search through uploaded documents
- **Computer Use** - Interact with computer interfaces

### Usage Pattern

Provider-native tools are passed through the `params` object, not the `tools` array:

```text
import openai from "upp/openai"

// Provider-native tools go in params.tools
gpt = llm({
  model: openai("gpt-4o"),
  params: {
    tools: [
      { type: "web_search" },
      { type: "image_generation", quality: "high" }
    ]
  },
  // UPP function tools go here (can be used together)
  tools: [myCustomTool]
})
```

### Output Handling

Provider-native tool outputs SHOULD be transformed into standard UPP content blocks:

- Image generation results → `ImageBlock` in `AssistantMessage.content`
- Audio generation results → `AudioBlock` in `AssistantMessage.content`
- Text-based results (web search, code output) → Incorporated into text response

This ensures outputs are accessible via standard message accessors:

```text
turn = await gpt.generate("Generate an image of a sunset")

// Generated images are standard content blocks
images = turn.response.images  // List<ImageBlock>
firstImage = images[0]
```

### Provider Implementation

Providers offering native tools SHOULD:

1. **Document available tools** - List supported native tools and their configuration options
2. **Export helper constructors** - Provide ergonomic functions for creating tool configurations
3. **Transform outputs to content** - Convert tool outputs to standard `ContentBlock` types
4. **Preserve tool metadata** - Store provider-specific execution details in `metadata.{providerName}` if needed for multi-turn or debugging

```text
// Example: Provider exports tool helpers
import { tools } from "upp/openai"

gpt = llm({
  model: openai("gpt-4o"),
  params: {
    tools: [
      tools.webSearch({ search_context_size: "medium" }),
      tools.imageGeneration({ quality: "high", size: "1024x1024" })
    ]
  }
})
```

### Combining Tool Types

UPP function tools and provider-native tools can be used together. The provider is responsible for merging them correctly when sending requests:

```text
// Both tool types in one configuration
gpt = llm({
  model: openai("gpt-4o"),
  params: {
    tools: [tools.webSearch()]  // Provider-native
  },
  tools: [getWeather, saveNote]  // UPP function tools
})

// The model can use any available tool
turn = await gpt.generate("What's the weather in Paris and save it to my notes")
```

The `llm()` core handles UPP function tool execution automatically. Provider-native tools execute server-side and their results appear in the response.

## Complex Tool Example

```text
searchDatabase = {
  name: "searchDatabase",
  description: "Search the product database",
  parameters: {
    type: "object",
    properties: {
      query: {
        type: "string",
        description: "Search query"
      },
      filters: {
        type: "object",
        properties: {
          category: {
            type: "string",
            enum: ["electronics", "clothing", "home", "sports"]
          },
          minPrice: { type: "number" },
          maxPrice: { type: "number" },
          inStock: { type: "boolean" }
        }
      },
      limit: {
        type: "integer",
        description: "Maximum results to return",
        default: 10
      },
      sortBy: {
        type: "string",
        enum: ["relevance", "price_low", "price_high", "newest"],
        default: "relevance"
      }
    },
    required: ["query"]
  },
  run: async (params) => {
    results = await db.products.search({
      query: params.query,
      filters: params.filters ?? {},
      limit: params.limit ?? 10,
      sort: params.sortBy ?? "relevance"
    })
    return JSON.stringify(results)
  }
}
```

## Tool Security Considerations

<Aside type="caution" title="Security Warning">
  Tools execute arbitrary code based on LLM-provided arguments:

  - Tool arguments MUST be treated as untrusted input
  - Implementations MUST NOT automatically validate arguments against schema
  - Tool implementations SHOULD validate and sanitize inputs
  - Sensitive operations SHOULD use approval handlers
  - File system and network operations SHOULD be sandboxed where possible
</Aside>

### Safe Tool Implementation

```text
readFile = {
  name: "readFile",
  description: "Read a file from the allowed directory",
  parameters: {
    type: "object",
    properties: {
      path: { type: "string", description: "Relative file path" }
    },
    required: ["path"]
  },
  run: async (params) => {
    // Validate and sanitize path
    safePath = path.normalize(params.path)

    // Check for path traversal
    if (safePath.includes("..") || path.isAbsolute(safePath)) {
      throw new Error("Invalid path: must be relative without traversal")
    }

    // Restrict to allowed directory
    fullPath = path.join(ALLOWED_DIR, safePath)
    if (!fullPath.startsWith(ALLOWED_DIR)) {
      throw new Error("Path outside allowed directory")
    }

    // Check file exists and is readable
    if (!await fs.exists(fullPath)) {
      throw new Error("File not found")
    }

    return await fs.readFile(fullPath, "utf-8")
  }
}
```

## Capability Check

Providers that support tools MUST set `capabilities.tools` to `true`. If tools are provided but the capability is `false`, `llm()` MUST throw `UPPError` with code `INVALID_REQUEST`.
