---
title: Architecture
description: Understanding UPP's unified provider architecture.
---

import { Aside } from '@astrojs/starlight/components';

# Architecture Overview

UPP provides a unified architecture that sits between your application code and vendor APIs.

## The Unified Provider Model

```
┌─────────────────────────────────────────────────────────────────┐
│                     Application Code                             │
└─────────────────────────────────────────────────────────────────┘
           │                    │                    │
           ▼                    ▼                    ▼
    ┌─────────────┐     ┌──────────────┐     ┌─────────────┐
    │    llm()    │     │  embedding() │     │   image()   │
    │             │     │              │     │             │
    └─────────────┘     └──────────────┘     └─────────────┘
           │                    │                    │
           ▼                    ▼                    ▼
    ┌─────────────────────────────────────────────────────────────┐
    │                  Provider Adapters                           │
    │  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐     │
    │  │anthropic │  │  openai  │  │  google  │  │stability │     │
    │  │  (LLM)   │  │(LLM,Emb, │  │(LLM,Emb, │  │ (Image)  │     │
    │  │          │  │  Image)  │  │  Image)  │  │          │     │
    │  └──────────┘  └──────────┘  └──────────┘  └──────────┘     │
    └─────────────────────────────────────────────────────────────┘
                              │
                              ▼
    ┌─────────────────────────────────────────────────────────────┐
    │                    Vendor APIs                               │
    │         (Anthropic, OpenAI, Google, Stability, etc.)        │
    └─────────────────────────────────────────────────────────────┘
```

## Data Flow

### LLM Data Flow

1. Developer calls `llm()` with a provider-bound model
2. Developer calls `generate()` or `stream()` with message history and new input
3. Provider transforms input to vendor-specific format
4. `llm()` core handles tool execution loop (if tools configured)
5. `llm()` core returns a `Turn` containing all messages from this inference
6. Developer appends turn messages to their history for future calls

### Embedding Data Flow

1. Developer calls `embedding()` with a provider-bound model to create an embedder instance
2. Developer calls `embed()` or `embedBatch()` on the embedder with text/content
3. Provider transforms input to vendor-specific format
4. Provider returns `Embedding` or `EmbeddingBatch` with vectors
5. Developer uses vectors for search, clustering, etc.

### Image Data Flow

1. Developer calls `image()` with a provider-bound model
2. Developer calls `generate()`, `edit()`, or `vary()` with prompt/images
3. Provider transforms input to vendor-specific format
4. Provider returns `ImageResult` with generated images
5. Developer saves or displays images

## Separation of Concerns

UPP separates configuration into distinct layers:

| Layer | Purpose | Shared Across Modalities |
|-------|---------|--------------------------|
| **Provider Config** | Infrastructure/connection settings | Yes |
| **Model Params** | Model behavior parameters | No (modality-specific) |
| **Modality Options** | Interface-specific settings | No |

```typescript
import { openai } from '@providerprotocol/ai/openai';

// Provider config is shared
const config: ProviderConfig = {
  apiKey: process.env.OPENAI_API_KEY,
  timeout: 30000,
  retryStrategy: new ExponentialBackoff({ maxAttempts: 3 }),
};

// Same openai() factory, different modalities
const chat = llm({
  model: openai('gpt-4o'),
  config,
  params: { temperature: 0.7, max_tokens: 4096 },  // LLM params
  system: 'You are a helpful assistant.',
  tools: [getWeather],
});

const embedder = embedding({
  model: openai('text-embedding-3-large'),
  config,
  params: { dimensions: 1536 },  // Embedding params
});

const imageGen = image({
  model: openai('dall-e-3'),
  config,
  params: { size: '1024x1024', quality: 'hd' },  // Image params
});
```

## Provider Structure

A provider exports a single factory function that returns a `ModelReference`. The same factory works with any modality—the model ID determines which handler is used:

```typescript
import { openai } from '@providerprotocol/ai/openai';
import { anthropic } from '@providerprotocol/ai/anthropic';
import { stability } from '@providerprotocol/ai/stability';

// Same openai() factory works with all modalities
llm({ model: openai('gpt-4o') });
embedding({ model: openai('text-embedding-3-small') });
image({ model: openai('dall-e-3') });

// Providers that only support one modality still use the same pattern
llm({ model: anthropic('claude-sonnet-4-20250514') });
image({ model: stability('stable-diffusion-xl-1024-v1-0') });
```

<Aside type="tip">
Internally, each provider combines modality handlers. See the [Provider Implementation Guide](/providers/implementation/) for details.
</Aside>

## ModelReference and Provider Types

```typescript
/**
 * A reference to a model, created by a provider factory.
 * Can be used with any use* function that the provider supports.
 */
interface ModelReference {
  /** The model identifier */
  readonly modelId: string;

  /** The provider that created this reference */
  readonly provider: Provider;
}

/**
 * A provider factory function with metadata and modality handlers.
 */
interface Provider<TOptions = unknown> {
  /** Create a model reference, optionally with provider-specific options */
  (modelId: string, options?: TOptions): ModelReference;

  /** Provider name */
  readonly name: string;

  /** Provider version */
  readonly version: string;

  /** Supported modalities */
  readonly modalities: {
    llm?: LLMHandler;
    embedding?: EmbeddingHandler;
    image?: ImageHandler;
  };
}
```

When a modality function receives a `ModelReference`, it:
1. Checks if the provider supports that modality
2. Throws `UPPError` with code `INVALID_REQUEST` if not supported
3. Binds the model ID to create the executable model
