---
title: Similarity Utilities
description: Computing similarity between embedding vectors.
---

import { Tabs, TabItem } from '@astrojs/starlight/components';

# Similarity Utilities

UPP provides optional utilities for computing similarity between embedding vectors.

## Import

```typescript
import {
  cosineSimilarity,
  euclideanDistance,
  dotProduct
} from '@providerprotocol/ai/similarity';
```

## Functions

### Cosine Similarity

Measures the cosine of the angle between two vectors. Returns a value between -1 and 1, where 1 means identical direction.

```typescript
const similarity = cosineSimilarity(embedding1.vector, embedding2.vector);
console.log(similarity); // 0.95 (very similar)
```

### Euclidean Distance

Measures the straight-line distance between two points. Lower values mean more similar.

```typescript
const distance = euclideanDistance(embedding1.vector, embedding2.vector);
console.log(distance); // 0.12 (very close)
```

### Dot Product

Computes the dot product of two vectors. Higher values generally indicate more similarity for normalized vectors.

```typescript
const product = dotProduct(embedding1.vector, embedding2.vector);
console.log(product); // 0.92
```

## Practical Examples

### Finding Similar Documents

```typescript
import { embedding } from '@providerprotocol/ai';
import { openai } from '@providerprotocol/ai/openai';
import { cosineSimilarity } from '@providerprotocol/ai/similarity';

const embedder = embedding({
  model: openai('text-embedding-3-large'),
});

async function findSimilar(
  query: string,
  documents: string[],
  topK: number = 5
) {
  // Embed the query
  const queryEmbedding = await embedder.embed(query);

  // Embed all documents
  const docEmbeddings = await embedder.embedBatch(documents);

  // Calculate similarities
  const similarities = docEmbeddings.embeddings.map((doc, index) => ({
    index,
    document: documents[index],
    similarity: cosineSimilarity(queryEmbedding.vector, doc.vector),
  }));

  // Sort by similarity (descending)
  similarities.sort((a, b) => b.similarity - a.similarity);

  // Return top K
  return similarities.slice(0, topK);
}

// Usage
const results = await findSimilar('How do I reset my password?', documents, 5);
for (const result of results) {
  console.log(`${result.similarity.toFixed(3)}: ${result.document}`);
}
```

### Semantic Search with Threshold

```typescript
async function semanticSearch(
  query: string,
  corpus: Array<{ id: string; text: string; vector: number[] }>,
  threshold: number = 0.7
) {
  const queryEmbedding = await embedder.embed(query);

  return corpus
    .map(doc => ({
      ...doc,
      similarity: cosineSimilarity(queryEmbedding.vector, doc.vector),
    }))
    .filter(doc => doc.similarity >= threshold)
    .sort((a, b) => b.similarity - a.similarity);
}
```

### Clustering by Similarity

<Tabs>
  <TabItem label="Simple Clustering">
```typescript
function groupBySimilarity(
  embeddings: Embedding[],
  threshold: number = 0.85
): Embedding[][] {
  const groups: Embedding[][] = [];
  const used = new Set<number>();

  for (let i = 0; i < embeddings.length; i++) {
    if (used.has(i)) continue;

    const group = [embeddings[i]];
    used.add(i);

    for (let j = i + 1; j < embeddings.length; j++) {
      if (used.has(j)) continue;

      const similarity = cosineSimilarity(
        embeddings[i].vector,
        embeddings[j].vector
      );

      if (similarity >= threshold) {
        group.push(embeddings[j]);
        used.add(j);
      }
    }

    groups.push(group);
  }

  return groups;
}
```
  </TabItem>
  <TabItem label="With K-Means">
```typescript
// For more advanced clustering, consider using a library like ml-kmeans
import kmeans from 'ml-kmeans';

async function clusterDocuments(documents: string[], k: number) {
  const batch = await embedder.embedBatch(documents);
  const vectors = batch.embeddings.map(e => e.vector);

  const result = kmeans(vectors, k);

  return documents.map((doc, i) => ({
    document: doc,
    cluster: result.clusters[i],
  }));
}
```
  </TabItem>
</Tabs>

## Which Metric to Use?

| Metric | Best For | Notes |
|--------|----------|-------|
| **Cosine Similarity** | Most semantic similarity tasks | Normalized, direction-based |
| **Euclidean Distance** | When magnitude matters | Absolute distance in space |
| **Dot Product** | Pre-normalized vectors | Fastest computation |

Most embedding models produce normalized vectors, making cosine similarity and dot product equivalent. **Cosine similarity is recommended** for general use.
