---
title: Batch Processing
description: Efficiently processing large numbers of embeddings.
---

import { Aside } from '@astrojs/starlight/components';

# Batch Processing

UPP provides multiple ways to process embeddings efficiently.

## embedBatch()

For small to medium batches that fit within provider limits:

```typescript
const batch = await embedder.embedBatch([
  'First document',
  'Second document',
  'Third document',
]);

console.log(batch.embeddings.length);  // 3
console.log(batch.usage.totalTokens);  // Total tokens used
```

## embedMany()

For large document sets with automatic batching:

```typescript
const documents = await loadDocuments(); // 10,000 documents

for await (const progress of embedder.embedMany(documents, {
  batchSize: 100,
  concurrency: 2,
})) {
  console.log(`Progress: ${progress.progress.percent}%`);

  // Process embeddings as they complete
  await storeInVectorDB(progress.embeddings);

  if (progress.done) {
    console.log('All embeddings complete');
  }
}
```

## EmbedManyOptions

```typescript
interface EmbedManyOptions {
  /** Maximum inputs per batch (default: provider limit) */
  batchSize?: number;

  /** Concurrency limit (default: 1) */
  concurrency?: number;

  /** Abort signal */
  signal?: AbortSignal;
}
```

## EmbeddingProgress

```typescript
interface EmbeddingProgress {
  /** Completed embeddings so far */
  embeddings: Embedding[];

  /** Progress information */
  progress: {
    completed: number;
    total: number;
    percent: number;
  };

  /** Set when all complete */
  done: boolean;
}
```

## Practical Example: Vector Database

```typescript
import { embedding } from '@providerprotocol/ai';
import { openai } from '@providerprotocol/ai/openai';

const embedder = embedding({
  model: openai('text-embedding-3-large'),
  params: { dimensions: 1536 },
});

async function indexDocuments(documents: string[]) {
  const vectors: Array<{ id: string; vector: number[]; text: string }> = [];

  for await (const progress of embedder.embedMany(documents, {
    batchSize: 100,
    concurrency: 3,
  })) {
    // Add each completed embedding to our collection
    for (const embedding of progress.embeddings) {
      vectors.push({
        id: crypto.randomUUID(),
        vector: embedding.vector,
        text: embedding.input as string,
      });
    }

    // Log progress
    console.log(
      `Indexed ${progress.progress.completed}/${progress.progress.total} ` +
      `(${progress.progress.percent.toFixed(1)}%)`
    );
  }

  // Insert all vectors into database
  await vectorDB.upsert(vectors);
  console.log(`Indexed ${vectors.length} documents`);
}
```

## Cancellation

Use an AbortSignal to cancel batch processing:

```typescript
const controller = new AbortController();

// Cancel after 30 seconds
setTimeout(() => controller.abort(), 30000);

try {
  for await (const progress of embedder.embedMany(documents, {
    signal: controller.signal,
  })) {
    await processEmbeddings(progress.embeddings);
  }
} catch (error) {
  if (error instanceof UPPError && error.code === 'CANCELLED') {
    console.log('Batch processing was cancelled');
  }
}
```

## Rate Limiting

UPP handles rate limiting automatically through retry strategies:

```typescript
import { ExponentialBackoff } from '@providerprotocol/ai';

const embedder = embedding({
  model: openai('text-embedding-3-large'),
  config: {
    retryStrategy: new ExponentialBackoff({
      maxAttempts: 5,
      baseDelay: 1000,
    }),
  },
});

// Rate limits are handled automatically
for await (const progress of embedder.embedMany(largeDocumentSet)) {
  // Processing continues even if rate limited
}
```

<Aside type="tip" title="Batch Size Recommendations">
- Start with provider defaults
- Increase batch size for throughput
- Decrease if hitting memory limits
- Use concurrency > 1 only if rate limits allow
</Aside>
